{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARIMAX\n",
    "_**Author**: Mahdi S. (DSI-NYC)_ \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [The 4 parts of time series](#The-4-parts-of-time-series)\n",
    "1. [SARIMA Workflow](#SARIMA-Workflow)\n",
    "    1. [Format time-series](#Australian-Tourism-Dataset)\n",
    "    1. [Visualize data](#Visualizing-data-(abridged-to-save-time))\n",
    "    1. [Determine Seasonality $S$](#Determining-Seasonality)\n",
    "        1. [Additive vs. Multiplicative Seasonality](#Additive-vs.-Multiplicative-Seasonality)\n",
    "        1. [Decomposing the data](#Decomposing-the-data)\n",
    "    1. [Ensure Stationarity (determine $d$ and $D$)](#Determine-$d$-and-$D$)\n",
    "    1. [Determine $p$ and $q$ and $P$ and $Q$](#Determine-$p$-and-$q$-and-$P$-and-$Q$)\n",
    "        1. [Using `auto_arima`](#Using-auto_arima)\n",
    "        1. [Using a custom time series grid search](#Using-a-custom-time-series-grid-search)\n",
    "    1. [Build model](#Modeling)\n",
    "    1. [Visualize model predictions](#Visualizing-Predictions)\n",
    "        1. [Using `.forecast`](#Using-.forecast)\n",
    "        1. [Using `.predict`](#Using-.predict)\n",
    "        1. [Using `.fittedvalues`](#Using-.fittedvalues)\n",
    "        1. [Using `.get_forecast` and `.get_prediction`](#Using-.get_forecast-and-.get_prediction)\n",
    "        1. [Model Scoring](#Model-Scoring)\n",
    "        1. [Multi-Step Out-of-Sample Forecasting](#Multi-Step-Out-of-Sample-Forecasting)\n",
    "1. [`SARIMAX`](#SARIMAX:-tossing-an-X-on-the-end-of-SARIMA)\n",
    "1. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to solve time-series problems\n",
    "1. ~~__Keep using a standard model (like linear regression)__ with engineered features related to time~~\n",
    "2. ~~__Basic ARIMA model__ using ONLY the target variable~~\n",
    "3. ~~__ARIMAX model__ using the target AND exogenous variables, which can include engineered features related to time~~\n",
    "4. __SARIMA model__ adding a seasonality component to ARIMA\n",
    "5. __SARIMAX model__ adding a seasonality component to ARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to install the package first\n",
    "```python\n",
    "!pip install pmdarima\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA / SARIMAX\n",
    "\n",
    "---\n",
    "With ARIMA we've captured trend and difference, but what about seasonality? None of the methods we've talked about yet include it. \n",
    "\n",
    "Enter seasonal ARIMA (or SARIMA). (The 'X' at the end is a python thing that indicates the model also supports exogenous variables.) It takes all the parameters of the base ARIMA model, with additional parameters (denoted $P, D, Q$ and $m$) to account for lags in season.\n",
    "\n",
    "So we denote the whole thing as: $SARIMA(p,d,q)(P,D,Q)m$ where $m$ indicates the number of timesteps in a season.\n",
    "\n",
    "A P=1 would make use of the first seasonally offset observation in the model, e.g. $t-(m*1)$ or $t-12$. A P=2, would use the last two seasonally offset observations $t-(m * 1)$, $t-(m * 2)$.\n",
    "\n",
    "Similarly, a D of 1 would calculate a first order seasonal difference and a Q=1 would use a first order errors in the model (e.g. moving average).\n",
    "\n",
    "More information about SARIMA can, of course, be found [in the documentation](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html).\n",
    "\n",
    "And [this tutorial](https://machinelearningmastery.com/sarima-for-time-series-forecasting-in-python/) from Dr. Jason Brownlee has a good overview of how to implement SARIMA in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 4 parts of time series\n",
    "\n",
    "We can think of every time series data point as having 4 main contributing parts to it, which was can decompose:\n",
    "- __Level:__ The baseline value for the series if it were a straight line.\n",
    "- __Trend:__ The optional and often linear increasing or decreasing behavior of the series over time.\n",
    "- __Seasonality:__ The optional repeating patterns or cycles of behavior over time.\n",
    "- __Noise:__ The optional variability in the observations that cannot be explained by the model.\n",
    "\n",
    "This is the theory behind how we ultimately approach time series data using the SARIMAX model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### SARIMA Workflow\n",
    "\n",
    "1. __Format time-series__\n",
    "1. Visualize data\n",
    "1. Determine Seasonality $S$\n",
    "1. Ensure Stationarity (determine $d$ and $D$)\n",
    "1. Determine $p$ and $q$ and $P$ and $Q$\n",
    "1. Build model\n",
    "1. Visualize model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Australian Tourism Dataset\n",
    "\n",
    "https://www.kaggle.com/luisblanche/quarterly-tourism-in-australia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller # ADFuller Hypothesis test\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = [\"Quarter\", \"Trips\",\"Region\",\"State\",\"Purpose\"]\n",
    "tour = pd.read_csv(\"./datasets/tourism.csv\", usecols = desired_cols, parse_dates=[\"Quarter\"], index_col = \"Quarter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What's the time interval for our data?__\n",
    "\n",
    "_Answer:_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always sort!\n",
    "tour.sort_index(inplace = True, ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Uh-oh! We have some repeated time indices because each observation is NOT just a time period for one region/location.__\n",
    "\n",
    "For now, let's just focus on seasonality first (SARIMA) and drop the exogenous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tour_trips = tour[\"Trips\"]\n",
    "tour_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's resample, keeping things quarterly, and aggregate using mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tour_trips = tour_trips.resample(\"Q\").mean()\n",
    "tour_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Whoa! What happened to our quarterly index!?__\n",
    "\n",
    "_Answer:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### SARIMA Workflow\n",
    "\n",
    "1. Format time-series\n",
    "1. __Visualize data__\n",
    "1. Determine Seasonality $S$\n",
    "1. Ensure Stationarity (determine $d$ and $D$)\n",
    "1. Determine $p$ and $q$ and $P$ and $Q$\n",
    "1. Build model\n",
    "1. Visualize model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing data (abridged to save time)\n",
    "\n",
    "_(See the local linear time series lesson for a more complete data visualization example)_\n",
    "\n",
    "###  Generate line plot of Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot of timeline\n",
    "tour_trips.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### SARIMA Workflow\n",
    "\n",
    "1. Format time-series\n",
    "1. Visualize data\n",
    "1. __Determine Seasonality $S$__\n",
    "1. Ensure Stationarity (determine $d$ and $D$)\n",
    "1. Determine $p$ and $q$ and $P$ and $Q$\n",
    "1. Build model\n",
    "1. Visualize model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Seasonality\n",
    "\n",
    "Before we get into determining seasonality, let's dive a little deeper into the 2 main kinds of seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive vs. Multiplicative Seasonality\n",
    "\n",
    "Simply put, seasonality can present itself in two ways:\n",
    "1. __Additive:__ where over time the peaks and valleys keep the same magnitude over time.\n",
    "2. __Multiplicative:__ where over time the peaks and valleys increase/decrease in magnitude over time.\n",
    "\n",
    "The easiest way to determine which kind of seasonality you have is to plot the data over time:\n",
    "\n",
    "#### Additive Seasonality\n",
    "<img src=\"assets/adding.png\" width=\"300px\">\n",
    "\n",
    "#### Multiplicative Seasonality\n",
    "<img src=\"assets/multi.png\" width=\"300px\">\n",
    "\n",
    "Once we determine which kind of seasonality is at play in our data, we can proceed to decompose our data properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick plot of timeline\n",
    "tour_trips.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Take a look at our time series plot again. Which kind of seasonality do you think applies best?__\n",
    "\n",
    "_Answer:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An aside: order vs. seasonal order\n",
    "\n",
    "When we worked with the ARIMA model, we had to describe the parameters of the model using the \"order\" in the form `(p, d, q)`, which referred to the AR, I, and MA components, respectively.\n",
    "\n",
    "The SARIMA model still uses the same order as ARIMA, with an additional order known as the \"seasonal order\", which takes a similar form `(P, D, Q, S)`, which refers specifically to the seasonal components of the model. In this way, __SARIMA models effectively have TWO main models working under the hood__: (1) an ARIMA model and (2) an augmented ARIMA model tuned to work with seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the seasonal period ($S$)\n",
    "\n",
    "The seasonal period is just the frequency of the data in a standardized form.\n",
    "\n",
    "__Seasonal period or $S$ or `m`__ refers to the number of observations per seasonal cycle. This value generally requires subject matter knowledge of the data, but you can use the following guidelines to determine this value.\n",
    "\n",
    "Data|\tFrequency\n",
    ":-:|:-:\n",
    "Annual|\t1\n",
    "Quarterly|\t4\n",
    "Monthly\t|12\n",
    "Weekly\t|52\n",
    "Daily |7\n",
    "\n",
    "Since our data is quarterly:\n",
    "- $period = 4$\n",
    "- $S = 4$\n",
    "- `m = 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing the data\n",
    "\n",
    "Once we have a good idea of which kind of seasonality we have, we can use the `seasonal_decompose` function to __determine if seasonality is truly present in the data.__\n",
    "\n",
    "_NOTE: If you try `model = \"multiplicative\"` you'll notice very odd residuals, which should let you know something's not quite right using \"multiplicative\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose Trips using \"additive\" and a period of 4\n",
    "decomposed_result = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decomposition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WARNING: The seasonal decomposition will ALWAYS pull out a seasonal component. You must look at the scale and shape of the plot and determine if significant.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .describe() to get sense of scale\n",
    "tour_trips.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Interpretation:__ Based on the decomposition of our variable, the seasonal component appears to play a significat scale (scale is $\\pm 5$). Additionaly, it's common sense to consider trips as seasonal. Therefore, __we'll assume seasonality is playing a significant role in our data and we will proceed with a SARIMA model.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### SARIMA Workflow\n",
    "\n",
    "1. Format time-series\n",
    "1. Visualize data\n",
    "1. Determine Seasonality $S$\n",
    "1. __Ensure Stationarity (determine $d$ and $D$)__\n",
    "1. Determine $p$ and $q$ and $P$ and $Q$\n",
    "1. Build model\n",
    "1. Visualize model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine $d$ and $D$\n",
    "\n",
    "As usual, we'll just the Augmented Dickey-Fuller hypothesis test to ensure stationarity and determine $d$ and $D$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_dftest(target_column):\n",
    "    '''Returns the Test Statistic and p-value for Augmented Dickey-Fuller test on given target column'''\n",
    "    dftest = adfuller(target_column)\n",
    "    dfoutput = pd.Series(dftest[0:2], index=['Test Statistic','p-value'])\n",
    "    return dfoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data (d = 0)\n",
    "interpret_dftest(tour_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once differenced data (d = 1)\n",
    "interpret_dftest(tour_trips.diff().dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've determined $d = 1$ and $D = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### SARIMA Workflow\n",
    "\n",
    "1. Format time-series\n",
    "1. Visualize data\n",
    "1. Determine Seasonality $S$\n",
    "1. Ensure Stationarity (determine $d$ and $D$)\n",
    "1. __Determine $p$ and $q$ and $P$ and $Q$__\n",
    "1. Build model\n",
    "1. Visualize model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine $p$ and $q$ and $P$ and $Q$\n",
    "\n",
    "We COULD look at the ACF and PACF graphs to help determine the AR and MA components for our orders. With enough practice and experience, we can elucidate these values directly (difficult - see local ARIMA lesson)...however, we could also use grid searching (easier).\n",
    "\n",
    "Grid Search options:\n",
    "- using `auto_arima`, which is a pre-made SARIMAX function for gridsearching both ARIMA and SARIMAX orders\n",
    "- using a custom grid search function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `auto_arima`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to install the package first\n",
    "```python\n",
    "!pip install pmdarima\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preparation\n",
    "\n",
    "Just like `GridSearchCV` we must be mindful not to leak information from our testing test into the searching process, so let's split our data now.\n",
    "\n",
    "\n",
    "#### A few considerations:\n",
    "1. Only using target (we won't be using exogenous variables, yet...)\n",
    "1. `train_test_split` works with just a single Series\n",
    "1. `shuffle = False` for time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY using the target\n",
    "y = tour_trips # for clarity\n",
    "\n",
    "# NOTICE shuffle = False\n",
    "y_train, y_test = train_test_split(y, test_size = .2, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Sometimes, you have to do away with train/test splits! Time series data is often very small compared to the data sets we're used to. We may not always have the luxury of reserving data into a validation/test set._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepwise_sarima = auto_arima(\n",
    "    y_train, # data\n",
    "    start_p = 0,\n",
    "    start_q = 0,\n",
    "    max_p = 3,\n",
    "    max_q = 3,\n",
    "    start_P = 0,\n",
    "    start_Q = 0,\n",
    "    max_P = 3,\n",
    "    max_Q = 3,\n",
    "    d = 1, # we know how much to difference, so don't need to waste time searching for this,\n",
    "    D = 1, # we know how much to difference, so don't need to waste time searching for this,\n",
    "    seasonal = True, # use seasonality!\n",
    "    m = 4, # MUST tell which seasonal period to use\n",
    "    trace=True,\n",
    "    error_action='ignore',  # don't want to know if an order does not work\n",
    "    suppress_warnings=True  # don't want convergence warnings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE:_\n",
    "- In general, the values for $p, q, P$ and $Q$ are typically low values - between 0 and 2, but can be more.\n",
    "- By default, `auto_arima` will optimize for [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion) to determine which values are best.\n",
    "- `auto_arima` returns a fitted model (the best found), much like `GridSearchCV`, however this is NOT a model from `statsmodels`. - it's from `pmdarima`\n",
    "- _We don't supply our differenced dataset - we supply the original dataset, since we're telling `auto_arima` that $d=1$ and $D=1$_\n",
    "- _We can still use some intuition from the ACF and PACF to guild our process - this takes time and experience working with time-series data (see local ARIMA lesson)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the best order?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the best seasonal order?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE: It's fairly common for $p = P$ and $q = Q$, though this will not always be the case._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check summary to ensure no problems\n",
    "stepwise_sarima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you find `nan` values in the coefficients, then something bad happened and you should reconsider your options:__\n",
    "1. Try another solver (depends on data - try `solver = \"powell\"`)\n",
    "1. Ensure stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some nice quick graphs of results\n",
    "stepwise_sarima.plot_diagnostics(figsize = (12, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a custom time series grid search\n",
    "\n",
    "_The following function was modified from the Global ARIMA lesson_\n",
    "\n",
    "Our strategy is this:\n",
    "\n",
    "1. Let's start AIC at some really, really big number and call it `best_aic`.\n",
    "1. We'll also instantiate `best_model`, `best_order`, and `best_seasonal_order`\n",
    "1. Create a nested for loop to iterate over our possible values of $p, q, P$ and $Q$.\n",
    "    1. At every combination of $p, q, P$ and $Q$, fit an SARIMA model.\n",
    "    1. If the SARIMA model has a better (lower) AIC than `best_aic`, then store the best order and seasonal order along with the model!\n",
    "1. At the end of the for loop, return the orders and the best model (lowest AIC).\n",
    "\n",
    "\n",
    "_NOTE: While we are not using exogenous variables yet, the `SARIMAX` model works just fine without it! So, we'll be using it without the \"X\"._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarima_grid_search(data, p_range = (0,2), q_range = (0,2), P_range = (0,2), Q_range = (0,2), d = 0, D = 0, S = 12, verbose = False):\n",
    "    best_aic = 99 * (10 ** 16) # a very large number\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    best_model = None\n",
    "    # Use nested for loop to iterate over values of p and q.\n",
    "    for p in range(p_range[0], p_range[1] + 1): # adding 1 so range is inclusive\n",
    "        for q in range(q_range[0], q_range[1] + 1): # adding 1 so range is inclusive\n",
    "            for P in range(P_range[0], P_range[1] + 1): # adding 1 so range is inclusive\n",
    "                for Q in range(Q_range[0], Q_range[1] + 1): # adding 1 so range is inclusiv\n",
    "                    # Insert try and except statements.\n",
    "                    try:\n",
    "                        # Fitting an SARIMA(p, d, q) & (P, D, Q, S) model.\n",
    "                        if verbose:\n",
    "                            print(f'Attempting to fit SARIMA({p},{d},{q}) & ({P},{D},{Q},{S})')\n",
    "                        order = (p,d,q) # values of p, d, q\n",
    "                        seasonal_order = (P, D, Q, S)\n",
    "                        # Instantiate SARIMA model.\n",
    "                        sarima = SARIMAX(endog = data.astype(float).dropna(), # endog = Y variable\n",
    "                                      order = order,\n",
    "                                      seasonal_order = seasonal_order) \n",
    "\n",
    "                        # Fit SARIMA model.\n",
    "#                         model = sarima.fit()\n",
    "                        # If getting Convergence Errors, try increasing maxiter and changing method\n",
    "                        model = sarima.fit(maxiter = 200, method = \"powell\", disp = 0) # disp = 0 b/c of excessive output\n",
    "                        \n",
    "\n",
    "                        if verbose:\n",
    "                            print(f'The AIC for SARIMA({p},{d},{q})  & ({P},{D},{Q},{S}) is: {model.aic}')\n",
    "\n",
    "                        # Is my current model's AIC better than our best_aic?\n",
    "                        if model.aic < best_aic:\n",
    "\n",
    "                            # If so, let's overwrite best found so far\n",
    "                            best_aic = model.aic\n",
    "                            best_order = order\n",
    "                            best_seasonal_order = seasonal_order\n",
    "                            best_model = model\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        pass\n",
    "\n",
    "    return {\n",
    "        \"best_aic\": best_aic, \n",
    "        \"best_order\" : best_order, \n",
    "        \"best_seasonal_order\" : best_seasonal_order, \n",
    "        \"best_model\": best_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out function on our data\n",
    "sarima_gd_result = sarima_grid_search(y_train, d = 1, D = 1, S = 4, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An aside: Convergence Warnings\n",
    "\n",
    "When estimating the coefficients, your model may not be able to converge and will give you the dreaded `ConvergenceWarning`\n",
    "\n",
    "If this happens, I would suggest 2 remedies:\n",
    "1. Increase your `maxiter` (default is 50)\n",
    "2. Change the `method` (default is 'lbfgs')\n",
    "\n",
    "For example:\n",
    "```python\n",
    "...\n",
    "fitted_model = model.fit(maxiter = 200, method = \"powell\")\n",
    "...\n",
    "```\n",
    "\n",
    "_NOTES:_\n",
    "- _If the likelihood function is flat, and/or has many local minima the \"powell\" method (which is a derivative-free method) helps._\n",
    "- _SARIMAX is a \"blunt\" tool and will have trouble honing in on high frequency (granular) data. If your data is too granular, consider downsampling._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check result\n",
    "sarima_gd_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE: your custom grid search results may disagree with the `auto_arima` as they handle the task in different ways. As you get more familiar with working with time-series data, you'll gain more insight in determining the AR and MA components._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### SARIMA Workflow\n",
    "\n",
    "1. Format time-series\n",
    "1. Visualize data\n",
    "1. Determine Seasonality $S$\n",
    "1. Ensure Stationarity (determine $d$ and $D$)\n",
    "1. Determine $p$ and $q$ and $P$ and $Q$\n",
    "1. __Build model__\n",
    "1. Visualize model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Notes:\n",
    "1. Use `enforce_stationarity = False` since we tell the model what the proper differencing is using $d$ and $D$\n",
    "1. We can set `freq` as quarterly, `\"Q\"`. While this is not entirely necessary, it serves as a good check to being explicit with your intention for what the expected input should be\n",
    "1. If you're having convergence warnings, try:\n",
    "    1. Increasing `maxiter`\n",
    "    1. Changing the solver (for example: `method = \"powell\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (2, 1, 0)\n",
    "seasonal_order = (3, 1, 0, 4)\n",
    "\n",
    "# instantiate\n",
    "tour_sarima = SARIMAX(\n",
    "    y_train, \n",
    "    order = order, \n",
    "    seasonal_order = seasonal_order, \n",
    "    freq = \"Q\", # freq helps set the index column of predictions\n",
    "    enforce_stationarity = False # ensure model is not trying to enfore stationarity (use d and D)\n",
    ")\n",
    "\n",
    "# fit\n",
    "fitted_tour_sarima = tour_sarima.fit() # stats model returns fitted model\n",
    "\n",
    "# fit, in case of convergence warning\n",
    "# fitted_tour_sarima = tour_sarima.fit(maxiter = 200, method = \"powell\", disp = 0) # stats model returns fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always check summary for issues\n",
    "fitted_tour_sarima.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### SARIMA Workflow\n",
    "\n",
    "1. Format time-series\n",
    "1. Visualize data\n",
    "1. Determine Seasonality $S$\n",
    "1. Ensure Stationarity (determine $d$ and $D$)\n",
    "1. Determine $p$ and $q$ and $P$ and $Q$\n",
    "1. Build model\n",
    "1. __Visualize model predictions__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Predictions\n",
    "\n",
    "#### Creating `plot_SARIMA_predictions` visualization function\n",
    "The following cell contains a graphing function we'll use after getting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_SARIMA_predictions(y_train, y_test = None, train_preds = None, test_preds = None, lower_conf_int = None, upper_conf_int = None, title = \"Title\", ylabel = \"{y_label}\",xlabel = \"Data\"):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.title(title)\n",
    "\n",
    "    # train data\n",
    "    plt.plot(y_train.index, y_train, lw=1, color='purple', ls='solid', label='Train Data')\n",
    "\n",
    "    # train prediction\n",
    "    if train_preds is not None:\n",
    "        plt.plot(train_preds.index, train_preds, lw=1, color='green', ls='dashed', label='Training Predictions')\n",
    "\n",
    "    # test data\n",
    "    if y_test is not None:\n",
    "        plt.plot(y_test.index, y_test, lw=1, color='blue', ls='solid',label='Test Data')\n",
    "\n",
    "    # test predictions\n",
    "    if test_preds is not None:\n",
    "        plt.plot(test_preds.index, test_preds, lw=1, color='red', ls='dashed',label='Testing Predictions')\n",
    "\n",
    "    #  labels\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "\n",
    "    # confidence intervals\n",
    "    if lower_conf_int is not None and upper_conf_int is not None:\n",
    "        plt.fill_between(y_test.index, lower_conf_int, upper_conf_int, color='k', alpha=0.1,  label =\"95% conf. int.\");\n",
    "    # legend\n",
    "    plt.legend(loc = \"upper left\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with.... `.forecast` vs. `.predict` vs. `.fittedvalues` vs. `.get_forecast`\n",
    "\n",
    "The SARIMA models have a few ways of making predictions. We'll cover some of the nuances here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.forecast`\n",
    "\n",
    "`.forecast` does what it sounds like it does. It has one main parameter `steps`, which corresponds to the number of time intervals to predict AFTER the end of the training set.\n",
    "\n",
    "However, this version of `.forecast` does not work like it's ARIMA counterpart. The SARIMAX `.forecast` method returns only the forecasted values (no standard error or confidence intervals)\n",
    "\n",
    "__You might imagine, we'd be interested in forecasting the length of the testing set, which will serve as a way of testing our model's predictions.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast len(y_test)\n",
    "num_steps = len(y_test)\n",
    "test_forecast = fitted_tour_sarima.forecast(steps = num_steps)\n",
    "\n",
    "test_forecast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting out-of-sample\n",
    "plot_SARIMA_predictions(\n",
    "    y_train, \n",
    "    y_test, \n",
    "    test_preds = test_forecast,\n",
    "    title = \"Average Quarterly Trips using out-of-sample forecast\",\n",
    "    ylabel = \"Average Trips\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE: `.forecast` by default works __\"out-of-sample\" meaning the prediction from one step is used to determine the prediction on the next step.___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.predict`\n",
    "\n",
    "`.predict` works much like how we're used to working with predictions.\n",
    "\n",
    "There are 3 main parameters to consider:\n",
    "- `start` = the index from the __training set__ to start predicting from (can also be a datetime string or a 'datetime' type)\n",
    "- `end`   = the index from the __training set__ to stop predicting at (can also be a datetime string or a 'datetime' type)\n",
    "- `dynamic` = to __predict \"in-sample\" (False : default)__ or \"out-of-sample\" (True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An aside: using correct length of predictions \n",
    "\n",
    "Recall that \"under the hood\" the model is differencing our data. __When we set $d = 1$ and $D = 1$, we must keep in mind that we lose the first observation__ from the training set. So, when we call on our model to make predictions, we must likewise start at 1 more than the starting index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating testing predictions\n",
    "test_preds = fitted_tour_sarima.predict(start = len(y_train), end = len(y_train) + len(y_test) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note the `len(y_train) + len(y_test) - 1` - subtracting 1 keeps the test lengths in alignement_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting in-sample\n",
    "plot_SARIMA_predictions(\n",
    "    y_train, \n",
    "    y_test,  \n",
    "    test_preds = test_preds,\n",
    "    title = \"Average Quarterly Trips using in-sample predictions\",\n",
    "    ylabel = \"Average Trips\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why do our predictions look like the \"out-of-sample\" forecast from before?__\n",
    "\n",
    "_Answer:_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.fittedvalues`\n",
    "\n",
    "`.fittedvalues` is actually an attribute (not a method) and refers to the internally stored training predictions.\n",
    "\n",
    "That's right, you don't have to `.predict` the training set!\n",
    "\n",
    "However, we must keep in mind the need to consider the affect of differencing on our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out fitted_tour_sarima.fittedvalues\n",
    "fitted_tour_sarima.fittedvalues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the first observation is 0. This is to keep the length the same as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of training dataset:\", len(y_train))\n",
    "print(\"Length of .fittedvalues:\", len(fitted_tour_sarima.fittedvalues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used these values as is, we'd get some bad graphs and a poor result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting .fittedvalues (wrong)\n",
    "plot_SARIMA_predictions(\n",
    "    y_train, \n",
    "    train_preds = fitted_tour_sarima.fittedvalues, \n",
    "    title = \"Average Quarterly Trips using internal .fittedvalues\",\n",
    "    ylabel = \"Average Trips\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the 0 at the start of the graph makes it seem like our model was very bad at predicting on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We MUST take into account the differencing taking place. In our case, we had a value of $d = 1$ and $D = 1$, so we lost the first value in our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting .fittedvalues (correct)\n",
    "plot_SARIMA_predictions(\n",
    "    y_train, \n",
    "    train_preds = fitted_tour_sarima.fittedvalues[1:], \n",
    "    title = \"Average Quarterly Trips using internal .fittedvalues\",\n",
    "    ylabel = \"Average Trips\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE: In practice, the training predictions for time-series models are less relevant, since the main concern it the validity of the model predictions AFTER training._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An aside: What about that early spike in the training set?\n",
    "\n",
    "Let's see what happend if we start the prediction process VERY early on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating EARLY testing predictions\n",
    "early_test_preds = fitted_tour_sarima.predict(start = 2, end = len(y_train) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting early in-sample predictions\n",
    "plot_SARIMA_predictions(\n",
    "    y_train = y_train[:2], \n",
    "    y_test = y_train[2:],\n",
    "    test_preds = early_test_preds,\n",
    "    title = \"Average Quarterly Trips using in-sample predictions\",\n",
    "    ylabel = \"Average Trips\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very telling! It shouldn't be surprising for the initial model predictions to be bad, since up until that early point, there is so little data to go off of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.get_forecast` and `.get_prediction`\n",
    "\n",
    "`.get_forecast` works in a similar way to how `.forecast` works - it uses 1 main parameter, `steps` and it makes forecasts \"out-of-sample\". However, `.get_forecast` does not return the forecasted values. Instead, it returns an object containing the forecasted values AND the confidence interval!\n",
    "\n",
    "Similarly, `.get_prediction` works like `.predict`, taking a `start`, `end`, etc. and returns an object we can extract the predicted values AND the confidence interval from!\n",
    "\n",
    "Let's use `.get_forecast` to get the test predictions AND their associated confidence interval.\n",
    "- The forecast values are contained in the object's `.predicted_mean` attribute\n",
    "    - _NOTE: these are the literal values as a numpy array, NOT a pandas Series, so you'll likely want to create your own Series with the associated datetime index_\n",
    "- The confidence intervals are returns from calling the `.conf_int()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get forecast object\n",
    "forecast_object = fitted_tour_sarima.get_forecast(steps = len(y_test))\n",
    "\n",
    "# grab forecasted values\n",
    "forecast_pred_values = forecast_object.predicted_mean\n",
    "# converting to pandas Seried with proper datetime index\n",
    "forecast_preds = pd.Series(forecast_pred_values, index = y_test.index)\n",
    "\n",
    "# grab confidence intervals\n",
    "pred_ci = forecast_object.conf_int()\n",
    "\n",
    "# break out lower and upper confidence values\n",
    "lower_conf_int = pred_ci.iloc[:,0]\n",
    "upper_conf_int = pred_ci.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting .get_forecast\n",
    "plot_SARIMA_predictions(\n",
    "    y_train, \n",
    "    y_test,\n",
    "    test_preds = forecast_preds,\n",
    "    lower_conf_int = lower_conf_int,\n",
    "    upper_conf_int = upper_conf_int,\n",
    "    title = \"Average Quarterly Trips using .get_forecast (out-of-sample)\",\n",
    "    ylabel = \"Average Trips\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why does the confidence interval get larger the further out the prediction goes?__\n",
    "\n",
    "_Answer:_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why is the confidence interval so important?__\n",
    "\n",
    "_Answer:_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train score (from .fittedvalues)\n",
    "r2_score(y_train[fitted_tour_sarima.fittedvalues[1:].index], fitted_tour_sarima.fittedvalues[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test score (from .forecast)\n",
    "r2_score(y_test[forecast_preds.index], forecast_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly our model was not able to handle the sudden change that occured in our test set.\n",
    "\n",
    "Also, the baseline model is quite good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test pred RSS \n",
    "np.sqrt(np.sum((y_test[forecast_preds.index] - forecast_preds)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline RSS\n",
    "np.sqrt(np.sum((y_test - np.mean(y_test))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our baseline model is surprisingly good on the test set, making it hard to beat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Also, notice we haven't done any cross validation! \n",
    "\n",
    "This is because __cross validation isn't much help for time-series data.__ At best, cross validation is tricky to set up for time series models, and at worst it's not applicable at all! \n",
    "\n",
    "Think about how that would even work...since everything is time dependent, we can't train our models on future data - that would cause leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Out-of-Sample Forecasting\n",
    "\n",
    "What is we simulated the real world?\n",
    "\n",
    "Say, for each quarter, we'll use all of the previous information we have to build our SARIMA model to make the prediction for JUST the next quarter.\n",
    "- For each observation in our test dataset:\n",
    "    - We'll build a new model using the previously accumulated data points to train on\n",
    "    - We'll forecast the next step and add the TRUE value of that step to the \"history\"\n",
    "- Repeat for each test observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the orders\n",
    "order          = (2, 1, 0)\n",
    "seasonal_order = (3, 1, 0, 4)\n",
    "\n",
    "# storing new test predictions, the history, and confidence intervals\n",
    "temp_test_preds = []\n",
    "history = y_train.values\n",
    "conf_ints = []\n",
    "\n",
    "# for each test observation...\n",
    "for val in y_test:\n",
    "    # create model\n",
    "    sarima = SARIMAX(history, order = order, seasonal_order = seasonal_order, enforce_stationarity = False)\n",
    "    # fit\n",
    "    fitted_sarima = sarima.fit(method = \"powell\", maxiter = 200, disp = 0) # stop excessive ouput with disp = 0\n",
    "    # make next out-of-sample prediction\n",
    "    pred_obj = fitted_sarima.get_forecast(steps = 1)\n",
    "    # add to preds and conf_ints\n",
    "    temp_test_preds.append(pred_obj.predicted_mean)\n",
    "    conf_ints.append(pred_obj.conf_int()[0])\n",
    "\n",
    "    # add TRUTH to history for next training\n",
    "    history = np.append(history, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up info for visualization\n",
    "new_test_preds = pd.Series(temp_test_preds, index = y_test.index)\n",
    "\n",
    "# converting list to numpy array for convenience\n",
    "lower_conf_int = np.array(conf_ints)[:,0]\n",
    "upper_conf_int = np.array(conf_ints)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting Multi-Step Out-of-Sample Forecasting\n",
    "plot_SARIMA_predictions(\n",
    "    y_train, \n",
    "    y_test,\n",
    "    test_preds = new_test_preds,\n",
    "    lower_conf_int = lower_conf_int,\n",
    "    upper_conf_int = upper_conf_int,\n",
    "    title = \"Average Quarterly Trips using Multi-Step Out-of-Sample Forecasting\",\n",
    "    ylabel = \"Average Trips\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows a more realistic view of how our model would perform \"in action\" during those months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Step Out-of-Sample Forecasting Test score\n",
    "r2_score(y_test, new_test_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to solve time-series problems\n",
    "1. ~~__Keep using a standard model (like linear regression)__ with engineered features related to time~~\n",
    "2. ~~__Basic ARIMA model__ using ONLY the target variable~~\n",
    "3. ~~__ARIMAX model__ using the target AND exogenous variables, which can include engineered features related to time~~\n",
    "4. ~~__SARIMA model__ adding a seasonality component to ARIMA~~\n",
    "5. __SARIMAX model__ adding a seasonality component to ARIMAX\n",
    "6. __Vector AutoRegressive (VAR) model__ if you need to predict using other predictions as input (akin to transfer learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX: tossing an X on the end of SARIMA\n",
    "\n",
    "### Air Pollution Dataset\n",
    "\n",
    "These data are from refinery monitoring stations in Atchison Village, Richmond CA, United States.\n",
    "\n",
    "https://www.kaggle.com/nicapotato/pollution-in-atchison-village-richmond-ca\n",
    "\n",
    "_NOTE: For brevity, the following will be a VERY abridged workflow!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = [\"Date\", \"Wind Speed\", \"Benzene\",\"CS2\",\"SO2\", \"Toluene\", \"Xylene\", \"Ozone\"]\n",
    "\n",
    "air_pol = pd.read_csv(\"datasets/air_pollution.csv\", parse_dates=[\"Date\"], index_col = [\"Date\"], usecols = desired_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALWAYS sort time index\n",
    "air_pol.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling by day (too granular otherwise)\n",
    "air_pol = air_pol.resample(\"D\").mean()\n",
    "air_pol.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much data do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "air_pol.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "111 observations! That's barely anything at all!\n",
    "\n",
    "__We'll have to forego making a train/test split, in order to have any chance of maintaining a good model__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine $S$\n",
    "\n",
    "Since our data will be daily, $S = 7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick plot of target vs. time \n",
    "air_pol[\"Ozone\"].plot();\n",
    "plt.ylabel(\"Ozone (particles per million)\")\n",
    "plt.title(\"Five-minute average Ozone air levels, over daily periods\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine $d$ and $D$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data\n",
    "interpret_dftest(air_pol[\"Ozone\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d = 1\n",
    "air_diff_1 = air_pol[\"Ozone\"].diff().dropna()\n",
    "interpret_dftest(air_diff_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude $d = 1$ and $D = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = air_pol.drop(columns=[\"Ozone\"])\n",
    "y = air_pol[\"Ozone\"] \n",
    "\n",
    "# NO TRAIN/TEST SPLIT - too little data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `auto_arima` to determine other components\n",
    "\n",
    "_Notice how we'll need to provide our exogenous variables into the process._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax = auto_arima(\n",
    "    y, # data\n",
    "    exogenous = X, # INCLUDING EXOGENOUS VARIABLES\n",
    "    start_p = 0,\n",
    "    start_q = 0,\n",
    "    max_p = 3,\n",
    "    max_q = 3,\n",
    "    start_P = 0,\n",
    "    start_Q = 0,\n",
    "    max_P = 3,\n",
    "    max_Q = 3,\n",
    "    d = 1, # we know how much to difference, so don't need to waste time searching for this,\n",
    "    D = 1, # we know how much to difference, so don't need to waste time searching for this,\n",
    "    seasonal = True, # use seasonality!\n",
    "    m = 7, # MUST tell which seasonal period to use\n",
    "    trace=True,\n",
    "    error_action='ignore',  # don't want to know if an order does not work\n",
    "    suppress_warnings=True  # don't want convergence warnings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check orders\n",
    "print(\"order: \", sarimax.order)\n",
    "print(\"seasonal_order: \", sarimax.seasonal_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using custom grid search to determine other components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarimax_grid_search(endog, exog, p_range = (0,2), q_range = (0,2), P_range = (0,2), Q_range = (0,2), d = 0, D = 0, S = 12, verbose = False):\n",
    "    best_aic = 99 * (10 ** 16) # a very large number\n",
    "    best_order = None\n",
    "    best_seasonal_order = None\n",
    "    best_model = None\n",
    "    # Use nested for loop to iterate over values of p and q.\n",
    "    for p in range(p_range[0], p_range[1] + 1): # adding 1 so range is inclusive\n",
    "        for q in range(q_range[0], q_range[1] + 1): # adding 1 so range is inclusive\n",
    "            for P in range(P_range[0], P_range[1] + 1): # adding 1 so range is inclusive\n",
    "                for Q in range(Q_range[0], Q_range[1] + 1): # adding 1 so range is inclusiv\n",
    "                    # Insert try and except statements.\n",
    "                    try:\n",
    "                        # Fitting an SARIMAX(p, d, q) & (P, D, Q, S) model.\n",
    "                        if verbose:\n",
    "                            print(f'Attempting to fit SARIMAX({p},{d},{q}) & ({P},{D},{Q},{S})')\n",
    "                        order = (p,d,q) # values of p, d, q\n",
    "                        seasonal_order = (P, D, Q, S)\n",
    "                        # Instantiate SARIMA model.\n",
    "                        sarima = SARIMAX(endog = endog.astype(float).dropna(), # endog = Y variable\n",
    "                                         exog = exog.astype(float).dropna(), # INCLUDING EXOGENOUS VARIABLES\n",
    "                                      order = order,\n",
    "                                      seasonal_order = seasonal_order) \n",
    "\n",
    "                        # Fit SARIMAX model.\n",
    "                        # model = sarima.fit()\n",
    "                        # If getting Convergence Errors, try increasing maxiter and changing method\n",
    "                        model = sarima.fit(maxiter = 200, method = \"powell\", disp = 0) # disp = 0 b/c of excessive output\n",
    "\n",
    "                        if verbose:\n",
    "                            print(f'The AIC for SARIMAX({p},{d},{q})  & ({P},{D},{Q},{S}) is: {model.aic}')\n",
    "\n",
    "                        # Is my current model's AIC better than our best_aic?\n",
    "                        if model.aic < best_aic:\n",
    "\n",
    "                            # If so, let's overwrite best found so far\n",
    "                            best_aic = model.aic\n",
    "                            best_order = order\n",
    "                            best_seasonal_order = seasonal_order\n",
    "                            best_model = model\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        pass\n",
    "\n",
    "    return {\n",
    "        \"best_aic\": best_aic, \n",
    "        \"best_order\" : best_order, \n",
    "        \"best_seasonal_order\" : best_seasonal_order, \n",
    "        \"best_model\": best_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out function on our data (with exog)\n",
    "sarimax_gs_result = sarimax_grid_search(y, X, d = 1, D = 1, S = 7, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_gs_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a disagreement between the grid search methods, which can happen because they both approach the problem in different ways.\n",
    "\n",
    "This is where experience comes into play! __Being more familiar with time series models, order = `(0, 1, 1)` and seasonal_order = `(0, 1, 1, 7)` makes the most sense.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (0, 1, 1)\n",
    "seasonal_order = (0, 1, 1, 7)\n",
    "\n",
    "# instantiate\n",
    "air_sarimax = SARIMAX(\n",
    "    y,\n",
    "    X, # INCLUDING EXOGENOUS VARIABLES\n",
    "    order = order, \n",
    "    seasonal_order = seasonal_order,\n",
    "    freq = \"D\", # freq helps set the index column of predictions\n",
    "    enforce_stationarity = False # ensure model is not trying to enfore stationarity (use d and D)\n",
    ")\n",
    "\n",
    "# fit\n",
    "# fitted_air_sarima = air_sarima.fit() # stats model returns fitted model\n",
    "\n",
    "# fit, in case of convergence warning\n",
    "fitted_air_sarimax = air_sarimax.fit(maxiter = 200, method = \"powell\", disp = 0) # stats model returns fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check summary for issues\n",
    "fitted_air_sarimax.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions\n",
    "\n",
    "Some considerations this time:\n",
    "1. Since we did not conduct a train/test split, we'll need to pick an arbitrary date within the dataset to start forecasting from\n",
    "1. Since we trained on the entire dataset, the `.predict` method can now properly make \"in-sample\" predictions, since it has the TRUE values (no need to use multi-step out-of-sample forecasting!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting index: about 80% into dataset\n",
    "starting_index = round(len(y) * .8)\n",
    "starting_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting date\n",
    "start_date = y.index[starting_index]\n",
    "start_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions (that are actually in-sample!)\n",
    "air_test_preds = fitted_air_sarimax.predict(start = start_date, dynamic = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we didn't set an end date. The model will predict up until it runs out of training data, which is stored internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old visualization function doesn't work properly without train/test split\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Five-minute average Ozone air levels, over daily periods (in-sample)\")\n",
    "\n",
    "# train data\n",
    "plt.plot(y.index, y, lw=1, color='blue', ls='solid', label='Train Data')\n",
    "\n",
    "# test predictions\n",
    "plt.plot(air_test_preds.index, air_test_preds, lw=1, color='red', ls='dashed', label='Forecast')\n",
    "\n",
    "#  labels\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Ozone (particles per million)\")\n",
    "\n",
    "# legend\n",
    "plt.legend(loc = \"upper left\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a confidence interval!\n",
    "\n",
    "We can use `.get_prediction`, in much the same way we used `.get_forecast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = y.index[-1]\n",
    "\n",
    "prediction_obj = fitted_air_sarimax.get_prediction(start_date, end_date, dynamice = False)\n",
    "\n",
    "# Test predictions (that are actually in-sample!)\n",
    "new_air_test_preds = prediction_obj.predicted_mean\n",
    "\n",
    "# Test prediction confidence intervals\n",
    "air_conf_ints = prediction_obj.conf_int()\n",
    "air_lower_ints = air_conf_ints.iloc[:,0]\n",
    "air_upper_ints = air_conf_ints.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old visualization function doesn't work properly without train/test split\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Five-minute average Ozone air levels, over daily periods (in-sample)\")\n",
    "\n",
    "# train data\n",
    "plt.plot(y.index, y, lw=1, color='blue', ls='solid', label='Train Data')\n",
    "\n",
    "# test predictions\n",
    "plt.plot(new_air_test_preds.index, new_air_test_preds, lw=1, color='red', ls='dashed', label='Forecast')\n",
    "\n",
    "#  labels\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Ozone (particles per million)\")\n",
    "\n",
    "# confidence intervals\n",
    "plt.fill_between(new_air_test_preds.index, air_lower_ints, air_upper_ints, color='k', alpha=0.1, label =\"95% conf. int.\");\n",
    "# legend\n",
    "plt.legend(loc = \"upper left\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Test\" score\n",
    "r2_score(y[start_date:], new_air_test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using RMSE, the language of stakeholders\n",
    "(mean_squared_error(y[start_date:], new_air_test_preds)) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Interpretation:__ When forecasting Ozone levels (one step ahead), our model is off by about $\\pm 5$ parts per million."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about making forecasts with SARIMAX?\n",
    "\n",
    "Our model was training to use exogenous variables, which means we MUST provide them when making predictions and forecasts outside of our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We MUST provide some exogenous variables\n",
    "# fitted_air_sarimax.forecast(steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next date should be 2015-11-19. \n",
    "\n",
    "Let's provide the first row of data, just to demonstrate the concept.\n",
    "\n",
    "___WARNING: When you're making real forecasts, you should be providing the proper exogenous variables!___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We MUST provide some exogenous variables\n",
    "fitted_air_sarimax.forecast(steps = 1, exog = X.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ways to solve time-series problems\n",
    "1. ~~__Keep using a standard model (like linear regression)__ with engineered features related to time~~\n",
    "2. ~~__Basic ARIMA model__ using ONLY the target variable~~\n",
    "3. ~~__ARIMAX model__ using the target AND exogenous variables, which can include engineered features related to time~~\n",
    "4. ~~__SARIMA model__ adding a seasonality component to ARIMA~~\n",
    "5. ~~__SARIMAX model__ adding a seasonality component to ARIMAX~~\n",
    "6. __Vector AutoRegressive (VAR) model__ if you need to predict using other predictions as input (akin to transfer learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Akaike Information Criterion (AIC)](https://en.wikipedia.org/wiki/Akaike_information_criterion)\n",
    "- [AIC vs. BIC](https://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other)\n",
    "- [Determine `m`](https://alkaline-ml.com/pmdarima/tips_and_tricks.html#understand-p-d-q-and-m)\n",
    "- [Determine seasonal period](https://robjhyndman.com/hyndsight/seasonal-periods/)\n",
    "- [Handling SARIMAX Convergence Warnings](https://stats.stackexchange.com/questions/313426/mle-convergence-errors-with-statespace-sarimax/313811)\n",
    "- [3 facts about time series forecasting that surprise experienced machine learning practitioners](https://towardsdatascience.com/3-facts-about-time-series-forecasting-that-surprise-experienced-machine-learning-practitioners-69c18ee89387)\n",
    "- [Forecasting with `auto_arima`](https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week4-LearningObjectives.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
